
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>4.1. Sự đánh đổi giữa độ chệch và phương sai &#8212; Deep AI KhanhBlog</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/my.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"TeX": {"Macros": {"N": "\\mathbb{N}", "floor": ["\\lfloor#1\\rfloor", 1], "bmat": ["\\left[\\begin{array}"], "emat": ["\\end{array}\\right]"]}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="https://phamdinhkhanh.github.io/deepai-book/ch_ml/OvfAndUdf.html" />
    <link rel="shortcut icon" href="../_static/logo.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="5. Thước đo mô hình phân loại" href="index_ModelMetric.html" />
    <link rel="prev" title="4. Độ chệch (bias) và phương sai (variance)" href="index_OvfAndUdf.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/ML_course_logos.jpeg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Deep AI KhanhBlog</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Lời nói đầu
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Giới thiệu
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../contents.html">
   Các chương dự kiến
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ch_intro/main_contents.html">
   Mục tiêu cuốn sách
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../latex.html">
   Latex
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../grossary.html">
   Bảng thuật ngữ
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Phụ lục
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch_appendix/appendix_dtypes.html">
   1. Định dạng dữ liệu
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch_appendix/appendix_dtypes_basic.html">
     1.1. Các định dạng số, boolean và ký tự
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch_appendix/index_pandas.html">
   2. Pandas
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch_appendix/appendix_pandas.html">
     2.1. Khởi tạo dataframe
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch_appendix/index_numpy.html">
   3. Numpy
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch_appendix/appendix_numpy.html">
     3.1. Khởi tạo một mảng trên numpy
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch_appendix/index_matplotlib.html">
   4. Matplotlib
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch_appendix/appendix_matplotlib.html">
     4.1. Format chung của một biểu đồ trên matplotlib
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch_appendix/index_OOP.html">
   5. Lập trình hướng đối tượng (Object Oriented Programming - OOP)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch_appendix/appendix_OOP.html">
     5.1. Class và Object
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch_appendix/index_pipeline.html">
   6. Sklearn Pipeline
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch_appendix/appendix_pipeline.html">
     6.1. Thiết kế pipeline
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch_appendix/index_Convex_Opt.html">
   7. Giới thiệu chung về optimization
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch_appendix/appendix_Convex_Opt.html">
     7.1. Bài toán dạng tổng quát
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Đại số tuyến tính
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../ch_algebra/appendix_algebra.html">
   1. Đại số tuyến tính
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Giới thiệu
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../ch_calculus/appendix_calculus.html">
   1. Giải tích tích phân
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Xác suất
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../ch_probability/appendix_probability.html">
   1. Xác suất
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Machine Learning
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="index_MLIntroduce.html">
   1. Khái quát Machine Learning
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="index_prediction.html">
   2. Bài toán dự báo
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="prediction.html">
     2.1. Ứng dụng của hồi qui tuyến tính
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="index_RidgedRegression.html">
   2.2. Hồi qui Ridge và Lasso
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="RidgedRegression.html">
     2.2.2. Hồi qui Ridge
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="index_classification.html">
   3. Bài toán phân loại
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="classification.html">
     3.1. Hồi qui Logistic
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="index_OvfAndUdf.html">
   4. Độ chệch (
   <em>
    bias
   </em>
   ) và phương sai (
   <em>
    variance
   </em>
   )
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     4.1. Sự đánh đổi giữa độ chệch và phương sai
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="index_ModelMetric.html">
   5. Thước đo mô hình phân loại
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="modelMetric.html">
     5.1. Bộ dữ liệu
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="index_creditScorecard.html">
   6. Ứng dụng mô hình scorecard
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
  <label for="toctree-checkbox-13">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="creditScorecard.html">
     6.1. Phương pháp chuyên gia và mô hình
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="index_SVM.html">
   7. Giới thiệu về SVM
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
  <label for="toctree-checkbox-14">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="SVM.html">
     7.1. Hàm mất mát của SVM
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="index_DecisionTree.html">
   8. Khái niệm về cây quyết định
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
  <label for="toctree-checkbox-15">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="DecisionTree.html">
     8.1. Mô hình cây quyết định (
     <em>
      decision tree
     </em>
     )
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="index_RandomForest.html">
   9. Giới thiệu về mô hình rừng cây (
   <em>
    Random Forest
   </em>
   )
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
  <label for="toctree-checkbox-16">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="RandomForest.html">
     9.1. Ý tưởng của mô hình rừng cây
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="index_Bayes.html">
   10. Bạn là
   <em>
    Tần suất
   </em>
   (
   <em>
    Frequentist
   </em>
   ) hay
   <em>
    Bayesian
   </em>
   ?
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/>
  <label for="toctree-checkbox-17">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="NaiveBayes.html">
     10.1. Ước lượng hợp lý tối đa (
     <em>
      Maximum Likelihood Function - MLE
     </em>
     )
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="index_FeatureEngineering.html">
   11. Giới thiệu về feature engineering
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" type="checkbox"/>
  <label for="toctree-checkbox-18">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="FeatureEngineering.html">
     11.1. Feature Engineering
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="index_Boosting.html">
   12. Phương pháp tăng cường (
   <em>
    Boosting
   </em>
   )
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" type="checkbox"/>
  <label for="toctree-checkbox-19">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="Boosting.html">
     12.1. AdaBoosting
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="index_KMeans.html">
   13. k-Means Clustering
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" type="checkbox"/>
  <label for="toctree-checkbox-20">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="KMeans.html">
     13.1. Các bước của thuật toán k-Means Clustering
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="index_HierarchicalClustering.html">
   14. Hierarchical Clustering (
   <em>
    phân cụm phân cấp
   </em>
   )
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-21" name="toctree-checkbox-21" type="checkbox"/>
  <label for="toctree-checkbox-21">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="HierarchicalClustering.html">
     14.1. Chiến lược hợp nhất (
     <em>
      agglomerative
     </em>
     )
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="index_DBSCAN.html">
   15. DBSCAN
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-22" name="toctree-checkbox-22" type="checkbox"/>
  <label for="toctree-checkbox-22">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="DBSCAN.html">
     15.1. Phương pháp phân cụm dựa trên mật độ (
     <em>
      Density-Based Clustering
     </em>
     )
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="index_GMM.html">
   16. Gaussian Mixture Model
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-23" name="toctree-checkbox-23" type="checkbox"/>
  <label for="toctree-checkbox-23">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="GMM.html">
     16.1. Ước lượng MLE cho
     <em>
      phân phối Gaussian đa chiều
     </em>
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="index_PCA.html">
   17. Giảm chiều dữ liệu
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-24" name="toctree-checkbox-24" type="checkbox"/>
  <label for="toctree-checkbox-24">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="PCA.html">
     17.1. Phương pháp phân tích suy biến
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Đóng góp từ những tác giả khác
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../ch_donation/fubini_and_riemann.html">
   Tích phân Riemann và định lý Fubini
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ch_donation/information_theory.html">
   Lý thuyết thông tin
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        <a class="dropdown-buttons"
            href="../_sources/ch_ml/OvfAndUdf.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download notebook file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/ch_ml/OvfAndUdf.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/phamdinhkhanh/deepai-book"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/phamdinhkhanh/deepai-book/issues/new?title=Issue%20on%20page%20%2Fch_ml/OvfAndUdf.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        <a class="edit-button" href="https://github.com/phamdinhkhanh/deepai-book/edit/main/book/ch_ml/OvfAndUdf.md"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/phamdinhkhanh/deepai-book/main?urlpath=tree/book/ch_ml/OvfAndUdf.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   4.1. Sự đánh đổi giữa độ chệch và phương sai
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#qua-khop-overfitting-va-vi-khop-underfitting">
   4.2. Quá khớp (
   <em>
    Overfitting
   </em>
   ) và vị khớp (
   <em>
    Underfitting
   </em>
   )
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#nguyen-nhan-cua-qua-khop-va-vi-khop">
     4.2.1. Nguyên nhân của quá khớp và vị khớp
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#vi-du-ve-qua-khop-va-vi-khop">
     4.2.2. Ví dụ về quá khớp và vị khớp
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#anh-huong-cua-qua-khop-va-vi-khop-toi-tac-vu-du-bao">
     4.2.3. Ảnh hưởng của quá khớp và vị khớp tới tác vụ dự báo
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cach-xac-dinh-qua-khop-va-vi-khop">
     4.2.4. Cách xác định quá khớp và vị khớp.
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#xu-ly-hien-tuong-qua-khop-va-vi-khop">
     4.2.5. Xử lý hiện tượng quá khớp và vị khớp
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#xu-ly-hien-tuong-qua-khop">
   4.3. Xử lý hiện tượng quá khớp
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#phong-tranh-qua-khop-trong-cac-mo-hinh-machine-learning-truyen-thong">
     4.3.1. Phòng tránh quá khớp trong các mô hình machine learning truyền thống
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#giam-so-luong-bien-va-su-dung-mo-hinh-it-phuc-tap">
       4.3.1.1. Giảm số lượng biến và sử dụng mô hình ít phức tạp
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#phuong-phap-dieu-chuan-regularization">
       4.3.1.2. Phương pháp điều chuẩn (
       <em>
        Regularization
       </em>
       )
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#phong-tranh-qua-khop-trong-mang-no-ron">
     4.3.2 Phòng tránh quá khớp trong mạng nơ ron
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#phuong-phap-dung-som-early-stopping">
       4.3.2.1. Phương pháp dừng sớm (
       <em>
        Early stopping
       </em>
       )
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#phuong-phap-dropout-dropout">
       4.3.2.2. Phương pháp dropout (
       <em>
        Dropout
       </em>
       )
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#xu-ly-hien-tuong-vi-khop">
   4.4. Xử lý hiện tượng vị khớp
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bo-sung-du-lieu-cho-mo-hinh">
     4.4.1. Bổ sung dữ liệu cho mô hình
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tang-cuong-du-lieu-augumentation">
     4.4.2. Tăng cường dữ liệu (Augumentation)
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#su-dung-thuat-toan-phuc-tap-hon">
     4.4.3. Sử dụng thuật toán phức tạp hơn
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#tong-ket">
   4.5. Tổng kết
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bai-tap-tham-khao">
   4.6. Bài tập tham khảo
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#tai-lieu-tham-khao">
   4.7. Tài liệu tham khảo
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="su-danh-doi-giua-do-chech-va-phuong-sai">
<h1>4.1. Sự đánh đổi giữa độ chệch và phương sai<a class="headerlink" href="#su-danh-doi-giua-do-chech-va-phuong-sai" title="Permalink to this headline">¶</a></h1>
<p>Gỉa sử chúng ta có một tập dữ liệu huấn luyện gồm <span class="math notranslate nohighlight">\(n\)</span> điểm <span class="math notranslate nohighlight">\(D = \{(x_1, y_1), (x_2, y_2), \dots, (x_n, y_n) \}\)</span> và một hàm huấn luyện được ước lượng từ tập huấn luyện là <span class="math notranslate nohighlight">\(\hat{f}(x; D)\)</span>. Ở đây ký hiệu <span class="math notranslate nohighlight">\(\hat{f}(x; D)\)</span> để thể hiện rằng hàm này được hồi qui dựa vào tập dữ liệu huấn luyện <span class="math notranslate nohighlight">\(D\)</span>. Kỳ vọng của chúng ta là hàm <span class="math notranslate nohighlight">\(\hat{f}(x; D)\)</span> sẽ gần xấp xỉ hàm thực tế là <span class="math notranslate nohighlight">\(f(x)\)</span>. Hàm <span class="math notranslate nohighlight">\(f(x)\)</span> biểu diễn mối quan hệ <strong>thực</strong> giữa <span class="math notranslate nohighlight">\(x\)</span> và <span class="math notranslate nohighlight">\(y\)</span>. Đồng thời chúng ta chấp nhận một phần sai số nhiễu <span class="math notranslate nohighlight">\(\epsilon\)</span> giữa hàm <span class="math notranslate nohighlight">\(f(x)\)</span> và giá trị ground truth <span class="math notranslate nohighlight">\(y\)</span>. Đây là phần sai số <strong>luôn luôn</strong> tồn tại giữa mô hình dự báo và grouth truth. Hay nói cách khác, bất kì mô hình nào cũng sẽ có sai số nếu như dữ liệu là ngẫu nhiên và mối quan hệ giữa đầu vào <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> và đầu ra <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> không được sinh ra bởi một hàm số được chủ định trước. Chính vì không thể tránh khỏi nên sai số này được coi như là một thành phần sai số không thể giảm bớt (<em>irreducible error</em>). Chúng ta giả định chúng như là thành phần nhiễu có kỳ vọng bằng 0 và phương sai là <span class="math notranslate nohighlight">\(\sigma_{\epsilon}^2\)</span>. Như vậy:</p>
<div class="math notranslate nohighlight">
\[y_i = f(x_i) + \epsilon_i\]</div>
<p><strong>Tổng bình phương sai số</strong> giữa giá trị dự báo <span class="math notranslate nohighlight">\(\hat{f}(x; D)\)</span> và giá trị thực tế <span class="math notranslate nohighlight">\(y\)</span> được biểu diễn qua tổng của độ chệch (<em>bias</em>) và phương sai (<em>variance</em>) như sau:</p>
<div class="math notranslate nohighlight">
\[\mathbf{E}{[(y-\hat{f}(x; D))^2]} = \underbrace{\mathbf{E}[(\hat{f}(x; D)-f(x))^2]}_{\text{bias error}}+ \underbrace{\mathbf{E}[(\hat{f}(x; D)-\mathbf{E}(\hat{f}(x; D)))^2]}_{\text{variance error}} + \underbrace{\sigma^2_{\epsilon}}_{\text{irreduciable error}} \tag{1}\]</div>
<p>Thật vậy, để chứng minh công thức <span class="math notranslate nohighlight">\((1)\)</span> chúng ta dựa trên hằng đẳng thức:</p>
<div class="math notranslate nohighlight">
\[(a+b+c)^2 = a^2+b^2+c^2+2ab+2bc+2ac\]</div>
<p>Bên dưới là cách chứng minh đẳng thức <span class="math notranslate nohighlight">\((1)\)</span> dành cho bạn nào muốn hiểu sâu:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{eqnarray}
\mathbf{E}[(y-\hat{f}(x; D))^2] &amp; = &amp; \mathbf{E}[(f(x)+\epsilon - \hat{f}(x; D))^2] \\
&amp; = &amp; \mathbf{E}[(~\underbrace{f(x)-\mathbf{E}[{\hat{f}(x; D)}]}_{a}+ \underbrace{\mathbf{E}[{\hat{f}(x; D)}]- \hat{f}(x; D)}_{b}+\underbrace{\epsilon}_{c}~)^2] \\
&amp; = &amp; \mathbf{E}[(f(x)-\mathbf{E}[\hat{f}(x; D)])^2 + (\hat{f}(x; D)-\mathbf{E}[\hat{f}(x; D)])^2 + \epsilon^2 + \\ 
&amp; ~ &amp; 2\epsilon(f(x)-\mathbf{E}[\hat{f}(x; D)])+2(f(x)+\epsilon-\mathbf{E}[\hat{f}(x; D)])(\mathbf{E}[\hat{f}(x; D)]-\hat{f}(x; D))] \\
&amp; = &amp; \mathbf{E}[(f(x)-\mathbf{E}[\hat{f}(x; D)])^2] + \mathbf{E}[(\hat{f}(x; D)-\mathbf{E}[\hat{f}(x; D)])^2] + \mathbf{E}(\epsilon^2) + \\
&amp; ~ &amp; \underbrace{\mathbf{E}[2\epsilon(f(x)-\mathbf{E}[\hat{f}(x; D)])]}_{0}+\underbrace{\mathbf{E}[2(y-\mathbf{E}[\hat{f}(x; D)])(\mathbf{E}[\hat{f}(x; D)]-\hat{f}(x; D))]}_{0} \\
&amp; = &amp; \underbrace{\mathbf{E}[(f(x)-\mathbf{E}[\hat{f}(x; D)])^2]}_{\text{bias error}} + \underbrace{\mathbf{E}[(\hat{f}(x; D)-\mathbf{E}[\hat{f}(x; D)])^2]}_{\text{variance error}} + \sigma_{\epsilon}^2
\end{eqnarray}\end{split}\]</div>
<p>Mặt khác do <span class="math notranslate nohighlight">\(\epsilon\)</span> được coi như thành phần nhiễu nên độc lập với <span class="math notranslate nohighlight">\(f(x)\)</span> và <span class="math notranslate nohighlight">\(\hat{f}(x; D)\)</span>. Như vậy áp dụng công thức kỳ vọng tích bằng tích các kỳ vọng đối với các biến độc lập ta thu được một đẳng thức khá quan trọng: <span class="math notranslate nohighlight">\(\mathbf{E}[2\epsilon(f(x)-\mathbf{E}[\hat{f}(x; D)])]=2\mathbf{E}(\epsilon)\mathbf{E}[f(x)-\mathbf{E}(\hat{f}(x; D))] = 0\)</span>.</p>
<p>Ngoài ra:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{eqnarray}\mathbf{E}[2(y-\mathbf{E}[\hat{f}(x; D)])(\mathbf{E}[\hat{f}(x; D)]-\hat{f}(x; D))] &amp; = &amp; \mathbf{E}_{x, y, D}[2(y-\mathbf{E}[\hat{f}(x; D)])(\mathbf{E}[\hat{f}(x; D)]-\hat{f}(x; D))] \\
&amp; = &amp; \mathbf{E}_{x, y}[2(y-\mathbf{E}[\hat{f}(x; D)])\underbrace{\mathbf{E}_{D}[\mathbf{E}[\hat{f}(x; D)]-\hat{f}(x; D)]}_{0} = 0\end{eqnarray}\end{split}\]</div>
<p>Như vậy từ dòng 4 suy ra đẳng thức ở dòng 5 và thu được công thức <span class="math notranslate nohighlight">\((1)\)</span>.</p>
<p>Công thức <span class="math notranslate nohighlight">\((1)\)</span> còn được gọi là công thức <strong>phân rã độ chệch-phương sai</strong> (<em>bias-variance decomposition</em>). Thành phần phương sai nhiễu <span class="math notranslate nohighlight">\(\sigma_{\epsilon}^2\)</span> có độ lớn không đáng kể nên ta có thể xem như <strong>tổng bình phương sai số</strong> chỉ phục thuộc phần lớn vào <em>độ chệch</em> và <em>phương sai</em>. Sự đánh đổi giữa <em>độ chệch</em> và <em>phương sai</em> thể hiện qua: <code class="docutils literal notranslate"><span class="pre">đối</span> <span class="pre">với</span> <span class="pre">các</span> <span class="pre">mô</span> <span class="pre">hình</span> <span class="pre">có</span> <span class="pre">cùng</span> <span class="pre">mức</span> <span class="pre">độ</span> <span class="pre">sai</span> <span class="pre">số,</span> <span class="pre">nếu</span> <span class="pre">muốn</span> <span class="pre">một</span> <span class="pre">mô</span> <span class="pre">hình</span> <span class="pre">dự</span> <span class="pre">báo</span> <span class="pre">ít</span> <span class="pre">chệch</span> <span class="pre">hơn</span> <span class="pre">thì</span> <span class="pre">sẽ</span> <span class="pre">cần</span> <span class="pre">phương</span> <span class="pre">sai</span> <span class="pre">lớn</span> <span class="pre">hơn</span> <span class="pre">và</span> <span class="pre">ngược</span> <span class="pre">lại</span></code>.</p>
<p>Kết quả của một mô hình machine learning có thể rơi vào một trong bốn trường hợp giữa <em>độ chệch</em> và <em>phương sai</em> như hình bên dưới.</p>
<!-- ![](https://www.kdnuggets.com/wp-content/uploads/bias-and-variance.jpg) -->
<p><img alt="" src="../_images/bias-and-variance.jpg" /></p>
<p><strong>Hình 1:</strong> Các khả năng về <em>độ chệch</em> và <em>phương sai</em> của mô hình. Trong hình giả sử các điểm màu xanh là phân phối của giá trị dự báo và vòng tròng màu đỏ ở giữa thể hiện vùng lõi của phân phối ground truth, nơi mà các điểm ground truth có mật độ tập trung cao. Như vậy các mô hình có thể rơi vào:</p>
<ul class="simple">
<li><p>Độ chệch thấp, phương sai thấp (<em>Low bias, Low Variance</em>): Đây là trường hợp mô hình khớp tốt vì phân phối của giá trị dự báo trùng với phân phối của ground truth.</p></li>
<li><p>Độ chệch thấp, phương sai cao (<em>Low Bias, High Variance</em>): Đây là trường hợp các giá trị dự báo sẽ giao động qua lại xung quanh ground truth. Thông thường trường hợp này sẽ xảy ra hiện tượng quá khớp (<em>overfitting</em>) mà chúng ta sẽ tìm hiểu sau.</p></li>
<li><p>Độ chệch cao, phương sai thấp (<em>High Bias, Low Variance</em>): Đây là trường hợp mô hình dự báo bị chệch, phân phối của giá trị dự báo nằm khác xa so với phân phối của ground truth. Thông thường xảy ra khi lớp mô hình quá đơn giản. Các mô hình có đặc điểm này thường bị vị khớp (<em>underfitting</em>).</p></li>
<li><p>Độ chệch cao, phương sai cao (<em>High Bias, High Variance</em>): Đặc điểm này thường thấy ở những mô hình kém khi nó vừa bị chệch và vừa dao động. Trong trường hợp này mô hình cũng bị vị khớp.</p></li>
</ul>
<p>Độ chệch và phương sai là những nguyên nhân trực tiếp dẫn tới hai hiện tượng <em>quá khớp</em> và <em>vị khớp</em>. Khi đó mô hình sẽ không thể sử dụng trong thực tế vì tính kém chính xác của chúng khi dự báo trên những tập dữ liệu mới, chúng ta sẽ phải tìm cách khắc phục chúng. Tiếp theo chúng ta cùng tìm hiểu <em>quá khớp</em> và <em>vị khớp</em> là gì và phương pháp khắc phục chúng.</p>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="qua-khop-overfitting-va-vi-khop-underfitting">
<h1>4.2. Quá khớp (<em>Overfitting</em>) và vị khớp (<em>Underfitting</em>)<a class="headerlink" href="#qua-khop-overfitting-va-vi-khop-underfitting" title="Permalink to this headline">¶</a></h1>
<p>Quá khớp (<em>Overfitting</em>) và vị khớp (<em>underfitting</em>) là những hiện tượng ảnh hưởng nghiêm trọng đến hiệu suất của mô hình khiến chúng không thể áp dụng được vào thực tế.</p>
<p>Để minh hoạ cho hiện tượng quá khớp và vị khớp chúng ta sẽ cùng lấy ví dụ thực tiễn về phân loại ảnh chó và mèo. Giả sử tập dữ liệu huấn luyện của bạn thu thập được bao gồm 9000 ảnh chó mèo từ các tập dataset của Việt Nam. Sau đó bạn lên mạng và download thêm 1000 ảnh chó mèo của nước ngoài làm tập kiểm tra.</p>
<p><strong>Quá khớp</strong></p>
<p>Bạn xây dựng một mô hình phân loại có độ chính xác đạt 90% trên tập huấn luyện nhưng chỉ 50% trên tập kiểm tra. Như vậy mô hình của bạn đang xảy ra hiện tượng <em>quá khớp</em>.</p>
<p>Khi nói đến <em>quá khớp</em> là ta nói đến trường hợp mô hình dự báo tốt trên tập huấn luyện nhưng không dự báo tốt trên tập kiểm tra. Trong trường hợp quá khớp thì mô hình chỉ học tốt trên những tập dữ liệu có cùng phân phối với tập dữ liệu huấn luyện. Đối với các trường hợp khác phân phối với tập huấn luyện mà nó chưa được học, ví dụ như tập kiểm tra khác phân phối, thì sẽ không được dự báo tốt.</p>
<!-- ![](https://media.geeksforgeeks.org/wp-content/cdn-uploads/20190523171258/Overfitting2.png) -->
<p><img alt="" src="../_images/overfitting.png" /></p>
<p><strong>Hình 2:</strong> Ví dụ về <em>quá khớp</em> (ngoài cùng bên phải), <em>vị khớp</em> (đầu tiên) và <em>vừa vặn</em> (ở giữa). Đường biên phân chia của mô hình <em>quá khớp</em> có xu hướng phân loại tốt <strong>mọi điểm</strong> dữ liệu nhưng đó là một đường biên rất phức tạp (thể hiện yếu tố phương sai cao) và không khái quát về hình dạng như đường biên phân chia ở hình chính giữa. Trái lại, đường biên của mô hình <em>vị khớp</em> thì quá đơn giản (thể hiện yếu tố phương sai thấp) và do đó dẫn tới phân loại sai nhiều điểm dữ liệu. Trong cả ba mô hình thì đường biên phân chia ở giữa là có xu hướng phân chia một cách <strong>tổng quát</strong> và <strong>chính xác</strong> trên dữ liệu huấn luyện lẫn kiểm tra. Đây là mô hình có độ chệch thấp và phương sai thấp.</p>
<p><strong>Vị khớp</strong></p>
<p>Trong một kịch bản khác, mô hình của bạn đạt chính xác 55% trên tập huấn luyện và 50% trên tập kiểm tra. Kết quả này là kém trên cả hai tập và ta kết luận mô hình đang xảy ra <em>vị khớp</em>.</p>
<p>Vị khớp là hiện tượng mà mô hình dự báo <strong>kém</strong> trên đồng thời cả tập huấn luyện và tập kiểm tra như hình ngoài cùng bên trái. Thông thường những mô hình quá đơn giản khi dự báo trên tập dữ liệu lớn thường dẫn tới hiện tượng vị khớp. Một mô hình vị khớp thì sẽ có độ chệch lớn nên các dự báo sẽ không thể chính xác và dẫn tới không thể áp dụng được mô hình vào thực tế. Lúc này chúng ta cần có chiến lược cải thiện mô hình trên cả hai khía cạnh <strong>lựa chọn mô hình tốt</strong> và <strong>cải thiện dữ liệu</strong>.</p>
<div class="section" id="nguyen-nhan-cua-qua-khop-va-vi-khop">
<h2>4.2.1. Nguyên nhân của quá khớp và vị khớp<a class="headerlink" href="#nguyen-nhan-cua-qua-khop-va-vi-khop" title="Permalink to this headline">¶</a></h2>
<p>Nguyên nhân của <em>quá khớp</em> có thể xuất phát từ mô hình quá phức tạp hoặc dữ liệu chưa đủ khái quát.</p>
<p>Những mô hình quá phức tạp thường có không gian biểu diễn lớn và dễ dàng khớp được những đường biên phân chia phức tạp. Điều này tưởng như là tốt cho quá trình dự báo nhưng hoá ra là không tốt vì mô hình phức tạp thì có xu hướng học chi tiết thay vì học qui luật tổng quát như những gì bạn đã thấy ở hình 2.</p>
<p>Hiện tượng <em>quá khớp</em> xuất phát từ dữ liệu cũng là hiện tượng khá phổ biến. Khi dữ liệu không đủ rộng và khái quát thì mô hình không thể dự báo tốt trên tập huấn luyện là những dữ liệu mà nó chưa được học. Lấy ví dụ về tác vụ phân loại ảnh chó và mèo ở trên. Trong dữ liệu huấn luyện hầu hết là ảnh các chú chó và mèo của Việt Nam nhưng trong dữ liệu kiểm tra lại là ảnh của những giống chó và mèo nước ngoài làm cho mô hình không dự báo đúng trên tập dữ liệu này.</p>
<p>Hiện tượng <em>vị khớp</em> cũng có thể xuất phát từ phía mô hình hoặc từ phía dữ liệu. Đối với những bộ dữ liệu lớn nhưng sử dụng mô hình quá nhỏ thì sẽ không đủ khả năng biểu diễn tốt dữ liệu. Chẳng hạn như hình ngoài cùng bên trái của hình 2 nếu chỉ sử dụng đường biên là một đường thẳng tuyến tính giản đơn thì không đủ sức mạnh để phân loại dữ liệu. Khi đó ta cần chuyển sang những lớp mô hình phức tạp hơn.</p>
<p>Dữ liệu không đủ đa dạng cũng là nguyên nhân dẫn tới hiện tượng <strong>vị khớp</strong>. Như trong ví dụ, để phân loại được ảnh chó và mèo nước ngoài thì chúng ta cần bổ sung thêm những dữ liệu mới vào tập huấn luyện chỉ gồm ảnh chó và mèo của Việt Nam. Quá trình bổ sung dữ liệu này cần phải được thực hiện định kỳ và liên tục trong suốt quá trình huấn luyện và áp dụng mô hình.</p>
</div>
<div class="section" id="vi-du-ve-qua-khop-va-vi-khop">
<h2>4.2.2. Ví dụ về quá khớp và vị khớp<a class="headerlink" href="#vi-du-ve-qua-khop-va-vi-khop" title="Permalink to this headline">¶</a></h2>
<p>Mục đích của ví dụ này nhằm chỉ ra cách sấp xỉ các hàm phi tuyến bằng phương pháp hồi qui đa thức (<em>Linear regression with Polynormial feature</em>) và mối liên hệ giữa sự thay đổi độ phức tạp đa thức dẫn tới các hiện tượng <em>quá khớp</em> và <em>vị khớp</em> như thế nào. Cụ thể chúng ta sẽ chỉ ra rằng khi mức độ phức tạp của các phương trình hồi qui đa thức càng gia tăng (tức bậc của đa thức càng cao) thì mô hình có xu hướng bị <em>quá khớp</em>. Đồng thời một phương trình đa thức quá giản đơn (chẳng hạn bậc 1) sẽ không khớp dữ liệu tốt, khi đó chúng ta gặp hiện tượng <em>vị khớp</em>.</p>
<p>Giả sử bộ dữ liệu của chúng ta có mối quan hệ giữa <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> và <span class="math notranslate nohighlight">\(y\)</span> theo phương trình cosin như bên dưới:</p>
<div class="math notranslate nohighlight">
\[y_i = \text{cos}(1.5\pi x_i) + \epsilon_i\]</div>
<p><span class="math notranslate nohighlight">\(x_i\)</span> nhận giá trị ngẫu nhiên và <span class="math notranslate nohighlight">\(\epsilon_i\)</span> đại diện cho sai số ngẫu nhiên. Khi đó đồ thị biểu diễn <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> theo <span class="math notranslate nohighlight">\(y\)</span> là một đường cong dạng hình sin.</p>
<p>Code mẫu này được tham khảo từ <a class="reference external" href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html"><em>quá khớp</em> and Underfitting - Sklearn</a>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">PolynomialFeatures</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>


<span class="k">def</span> <span class="nf">true_fun</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="mf">1.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">X</span><span class="p">)</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="n">n_train_samples</span><span class="p">,</span> <span class="n">n_test_samples</span> <span class="o">=</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">10</span>

<span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n_train_samples</span><span class="p">))</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">true_fun</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_train_samples</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.1</span>

<span class="nb">print</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">figure</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">9</span><span class="p">)):</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="n">figure</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;True function&#39;</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Samples&#39;</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">((</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;best&#39;</span><span class="p">)</span>
  <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;X and y relationship&#39;</span><span class="p">)</span>
  
<span class="n">plot</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(30,) (30,)
</pre></div>
</div>
<img alt="../_images/OvfAndUdf_8_1.png" src="../_images/OvfAndUdf_8_1.png" />
</div>
</div>
<p>Thành phần sai số <span class="math notranslate nohighlight">\(\epsilon_i\)</span> được thêm vào mô hình nên mối quan hệ giữa <span class="math notranslate nohighlight">\(x\)</span> và <span class="math notranslate nohighlight">\(y\)</span> là không hoàn toàn theo phương trình cosin. Hàm <span class="math notranslate nohighlight">\(\text{cos}(1.5\pi x_i)\)</span> chính là hàm thực tế giữa <span class="math notranslate nohighlight">\(x\)</span> và <span class="math notranslate nohighlight">\(y\)</span>, nó tương đương với hàm <span class="math notranslate nohighlight">\(f(x)\)</span> ở mục độ chệch và phương sai.</p>
<p>Tiếp theo chúng ta sẽ tìm cách xấp xỉ mối quan hệ giữa <span class="math notranslate nohighlight">\(x\)</span> và <span class="math notranslate nohighlight">\(y\)</span> thông qua hồi qui đa thức với các bậc cao nhất là <code class="docutils literal notranslate"><span class="pre">1,</span> <span class="pre">4,</span> <span class="pre">15</span></code> và đánh giá cross validation theo metric MSE cho từng trường hợp. Bạn đọc có thể tham khảo lại <a class="reference external" href="https://phamdinhkhanh.github.io/deepai-book/ch_appendix/appendix_pipeline.html#danh-gia-cheo-cross-validation">phụ lục - 6.2. Đánh giá cheó (cross validation)¶
</a> để hiểu thêm khái niệm. Khi kết quả MSE của cross validation càng lớn thì chứng tỏ mô hình gặp hiện tượng <em>quá khớp</em> càng nặng.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">degrees</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">15</span><span class="p">]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">degrees</span><span class="p">)):</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">degrees</span><span class="p">),</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">setp</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">xticks</span><span class="o">=</span><span class="p">(),</span> <span class="n">yticks</span><span class="o">=</span><span class="p">())</span>

    <span class="c1"># Tạo các Featuer bậc degrees[i] cho mô hình.</span>
    <span class="n">polynomial_features</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="n">degrees</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                                             <span class="n">include_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="c1"># Hồi qui tuyến tính</span>
    <span class="n">linear_regression</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
    
    <span class="c1"># Pipeline đơn giản cho mô hình từ feature engineering tới hồi qui tuyến tính</span>
    <span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([(</span><span class="s1">&#39;polynomial_features&#39;</span><span class="p">,</span> <span class="n">polynomial_features</span><span class="p">),</span>
                         <span class="p">(</span><span class="s1">&#39;linear_regression&#39;</span><span class="p">,</span> <span class="n">linear_regression</span><span class="p">)])</span>
    
    <span class="c1"># Huấn luyện mô hình</span>
    <span class="n">pipeline</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">],</span> <span class="n">y_train</span><span class="p">)</span>

    <span class="c1"># Đánh giá mô hình sử dụng cross validation</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">pipeline</span><span class="p">,</span> <span class="n">X_train</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">],</span> <span class="n">y_train</span><span class="p">,</span>
                             <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;neg_mean_squared_error&#39;</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

    <span class="c1"># Dự báo trên tập huấn luyện</span>
    <span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
    <span class="n">y_test_pred</span> <span class="o">=</span> <span class="n">pipeline</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">])</span>

    <span class="c1"># Vẽ biểu đồ trên tập huấn luyện</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test_pred</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Model&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">true_fun</span><span class="p">(</span><span class="n">X_test</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;True function&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Samples&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">((</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;best&#39;</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Degree </span><span class="si">{}</span><span class="se">\n</span><span class="s1">MSE = </span><span class="si">{:.2e}</span><span class="s1">(+/- </span><span class="si">{:.2e}</span><span class="s1">)&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
        <span class="n">degrees</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="o">-</span><span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">scores</span><span class="o">.</span><span class="n">std</span><span class="p">()))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/OvfAndUdf_10_0.png" src="../_images/OvfAndUdf_10_0.png" />
</div>
</div>
<p>Trong cả 3 mô hình trên thì bậc 15 có kết quả MSE cao nhất và đây cũng là mô hình bị <em>quá khớp</em> nặng nhất. Đa thức bậc 15 thì có khả năng biểu diễn tốt hơn các bậc 1 và 4 nhưng quy luật mà nó học được không khái quát quy luật chung nên phương sai MSE validation cao nhất.</p>
<p>Đường thẳng huấn luyện bậc 1 lại thể hiện xu hướng bị chệch khỏi phương trình gốc. Chúng ta có thể thấy tín hiệu chệch qua sự khác biệt giữa đường thẳng <code class="docutils literal notranslate"><span class="pre">Model</span></code> và đường <code class="docutils literal notranslate"><span class="pre">true</span> <span class="pre">function</span></code>. Giá trị MSE validation của chúng mặc dù thấp hơn bậc 15 nhưng do bị chệch nên vẫn còn cao.</p>
<p>Chỉ có đường cong tương ứng với bậc 4 là vừa khớp tốt qui luật tổng quát trên cả tập huấn luyện và kiểm tra. Điều này được thể hiện qua MSE validation là nhỏ nhất và hình dạng đường cong rất sát với đường <code class="docutils literal notranslate"><span class="pre">true</span> <span class="pre">function</span></code>. Như vậy phương trình bậc 4 sẽ là lựa chọn lý tưởng nhất cho mô hình hồi qui đa thức.</p>
</div>
<div class="section" id="anh-huong-cua-qua-khop-va-vi-khop-toi-tac-vu-du-bao">
<h2>4.2.3. Ảnh hưởng của quá khớp và vị khớp tới tác vụ dự báo<a class="headerlink" href="#anh-huong-cua-qua-khop-va-vi-khop-toi-tac-vu-du-bao" title="Permalink to this headline">¶</a></h2>
<p>Một phương trình xảy ra <em>quá khớp</em> hoặc <em>vị khớp</em> đều không tốt cho tác vụ dự báo. Hiện tượng <em>quá khớp</em> khiến mô hình chỉ khớp được tốt tập huấn luyện mà không khớp tốt các dữ liệu có phân phối khác huấn luyện. Hiện tượng <em>vị khớp</em> sẽ tạo ra một mô hình bị chệch trên cả tập huấn luyện và kiểm tra nên thường xảy ra lỗi dự báo. Như vậy cả hai hiện tượng <em>quá khớp</em> và <em>vị khớp</em> đều gây ra những ảnh hưởng tiêu cực và cần được phát hiện để khắc phục.</p>
<p>Phương pháp sửa lỗi mô hình khi xảy ra <em>quá khớp</em> và <em>vị khớp</em> là khác biệt nhau. Vì vậy khi xây dựng mô hình chúng ta cần phải xác định hiện tượng mà mô hình đang mắc phải là gì trong số chúng, sau đó mới lựa chọn ra phương pháp điều chỉnh phù hợp. Trong mục tiếp theo là giới thiệu sơ bộ cách xác định <em>quá khớp</em> và <em>vị khớp</em> và chiến lược để khắc phục những hiện tượng này.</p>
</div>
<div class="section" id="cach-xac-dinh-qua-khop-va-vi-khop">
<h2>4.2.4. Cách xác định quá khớp và vị khớp.<a class="headerlink" href="#cach-xac-dinh-qua-khop-va-vi-khop" title="Permalink to this headline">¶</a></h2>
<p>Để đánh giá <em>qúa khớp</em> và <em>vị khớp</em> chúng ta sẽ so sánh thước đo (<em>metric</em>) của mô hình trên đồng thời cả hai tập huấn luyện và kiểm tra và đối chiếu chúng.</p>
<p>Nếu thước đo cho thấy kết quả dự báo mô hình trên tập huấn luyện <strong>tốt hơn</strong> so với tập kiểm tra thì ta nói mô hình gặp hiện tượng <em>quá khớp</em>. Ở đây ta dùng từ <strong>tốt hơn</strong> có nghĩa là thước đo đó có thể lớn hơn hoặc nhỏ hơn tuỳ thuộc vào từng loại thước đo. Chẳng hạn như thước đo là độ chính xác (<em>accuracy</em>) thì tốt hơn nghĩa là lớn hơn, còn đối với sai số MSE thì tốt hơn đồng nghĩa với nhỏ hơn.</p>
<p>Chênh lệch giá trị thước đo trên tập huấn luyện và kiểm tra là điều không tránh khỏi. Xác suất để thước đo đánh giá trên tập huấn luyện và kiểm tra là bằng nhau dường như là 0%. Vì thế khi nói đến hiện tượng <em>quá khớp</em> xảy ra là chúng ta đang xét đến sai số của thước đo trên tập huấn luyện và tập kiểm tra là <strong>chênh lệch lớn</strong>. Ví dụ nếu thước đo độ chính xác trên tập huấn luyện đạt 95% nhưng tập kiểm tra chỉ đạt 50% thì ta có thể khẳng định mô hình đang gặp hiện tượng <em>quá khớp</em>. Nhưng nếu thước đo độ chính xác trên tập kiểm tra là 94% và không khác nhiều so với tập huấn luyện thì ta có thể coi như mô hình là vừa vặn.</p>
<p>Trong một khía cạnh khác, nếu <em>độ chính xác</em> trên tập huấn luyện và tập kiểm tra đều cùng thấp, ví dụ tập huấn luyện đạt 55%, tập kiểm tra 50% thì mô hình đang gặp hiện tượng <em>vị khớp</em>.</p>
</div>
<div class="section" id="xu-ly-hien-tuong-qua-khop-va-vi-khop">
<h2>4.2.5. Xử lý hiện tượng quá khớp và vị khớp<a class="headerlink" href="#xu-ly-hien-tuong-qua-khop-va-vi-khop" title="Permalink to this headline">¶</a></h2>
<p>Xử lý hiện tượng <em>quá khớp</em> và <em>vị khớp</em> là một trong những kỹ thuật quan trọng trong quá trình xây dựng mô hình. Những kỹ thuật này là kinh nghiệm được đúc rút từ thực nghiệm và các tài liệu khoa học đáng tin cậy như cuốn <a class="reference external" href="https://github.com/mlbvn/ml-yearning-vn">Machine Learning Yearning - Andrew Ng</a> để người xây dựng mô hình trở thành <code class="docutils literal notranslate"><span class="pre">master</span></code> khi tham gia phát triển các dự án AI thực tế.</p>
<p>Có nhiều phương pháp và kỹ thuật khác nhau để xử lý hiện tượng <em>quá khớp</em> và <em>vị khớp</em>, xong chúng đều xuất phát từ hai khía cạnh đó là tập trung vào mô hình (<em>model centric</em>) hoặc tập trung vào dữ liệu (<em>data centric</em>).</p>
<p>Tập trung vào mô hình là nhằm sử dụng những kiến trúc và thuật toán tốt hơn nữa để tăng hiệu suất mô hình. Ví dụ như trong bài toán phân loại của học có giám sát chúng ta có thể sử dụng những lớp mô hình có độ phức tạp cao hơn như <code class="docutils literal notranslate"><span class="pre">Random</span> <span class="pre">Forest,</span> <span class="pre">Decision</span> <span class="pre">Tree,</span> <span class="pre">SVM</span></code> thay cho những lớp mô hình độ phức tạp thấp như <code class="docutils literal notranslate"><span class="pre">Logistic</span> <span class="pre">Regression</span></code> như một cách tiếp cận theo hướng model centric.</p>
<p>Tập trung vào dữ liệu thường được sử dụng khi đã lựa chọn được một mô hình đủ tốt, những sự thay đổi về kiến trúc của mô hình không tạo ra cải thiện thêm về hiệu suất. Khi đó cần mở rộng bộ dữ liệu cả về <strong>chất lượng và số lượng</strong> để tạo ra những cải thiện đột phá cho mô hình. Một trường hợp khác mà cần áp tập trung vào dữ liệu ngay từ đầu đó là bộ dữ liệu có kích thước quá nhỏ và chất lượng của bộ dữ liệu không được tốt chẳng hạn như nhiều dữ liệu khuyết (<em>missing data</em>) và điểm ngoại lệ (<em>outliers</em>). Nếu huấn luyện mô hình trên những dữ liệu kém chất lượng như vậy thì sử dụng các kiến trúc SOTA cũng không mang lại hiệu quả.</p>
<p>Tiếp theo chúng ta cùng tìm hiểu các phương pháp khắc phục hiện tượng <em>quá khớp</em>.</p>
</div>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="xu-ly-hien-tuong-qua-khop">
<h1>4.3. Xử lý hiện tượng quá khớp<a class="headerlink" href="#xu-ly-hien-tuong-qua-khop" title="Permalink to this headline">¶</a></h1>
<div class="section" id="phong-tranh-qua-khop-trong-cac-mo-hinh-machine-learning-truyen-thong">
<h2>4.3.1. Phòng tránh quá khớp trong các mô hình machine learning truyền thống<a class="headerlink" href="#phong-tranh-qua-khop-trong-cac-mo-hinh-machine-learning-truyen-thong" title="Permalink to this headline">¶</a></h2>
<p>Nội dung của những phương pháp này được áp dụng trên những lớp mô hình machine learning truyền thống không bao gồm các mạng thần kinh nơ ron (<em>neural network</em>).</p>
<div class="section" id="giam-so-luong-bien-va-su-dung-mo-hinh-it-phuc-tap">
<h3>4.3.1.1. Giảm số lượng biến và sử dụng mô hình ít phức tạp<a class="headerlink" href="#giam-so-luong-bien-va-su-dung-mo-hinh-it-phuc-tap" title="Permalink to this headline">¶</a></h3>
<p>Như ví dụ về các bậc <code class="docutils literal notranslate"><span class="pre">1,</span> <span class="pre">4,</span> <span class="pre">15</span></code> chúng ta đã phân tích thì bậc <code class="docutils literal notranslate"><span class="pre">15</span></code> gặp hiện tượng <em>quá khớp</em> mặc dù nó khớp rất tốt các điểm trên tập huấn luyện. Đây là một minh chứng cho thấy chúng ta thường đối mặt với hiện tượng <em>quá khớp</em> khi xây dựng mô hình trên những bộ dữ liệu có kích thước nhỏ nhưng sử dụng những mô hình có độ phức tạp cao. Do đó một cách đơn giản để tránh <em>quá khớp</em> là giảm nhẹ độ phức tạp của mô hình bằng cách giảm bớt số lượng tham số, số lượng biến đầu vào và chuyển sang sử dụng những mô hình ít phức tạp hơn. Chẳng hạn trong ví dụ trên chúng ta chuyển từ bậc 15 sang bậc 4 thì mô hình đã không còn gặp hiện tượng <em>quá khớp</em> cao nữa.</p>
</div>
<div class="section" id="phuong-phap-dieu-chuan-regularization">
<h3>4.3.1.2. Phương pháp điều chuẩn (<em>Regularization</em>)<a class="headerlink" href="#phuong-phap-dieu-chuan-regularization" title="Permalink to this headline">¶</a></h3>
<p>Điều chuẩn cũng là một phương pháp nhằm giảm thiểu độ phức tạp của mô hình. Trong phương pháp điều chuẩn chúng ta tìm cách cộng thêm vào giá trị của hàm mất mát (<em>loss function</em>) một thành phần kiểm soát để làm cho mô hình có xu hướng học được một kết quả khái quát hơn trên bộ dữ liệu huấn luyện. Lấy ví dụ trong phương trình hồi qui tuyến tính giữa <span class="math notranslate nohighlight">\(x\)</span> và <span class="math notranslate nohighlight">\(y\)</span> chúng ta sẽ tìm cách tối ưu <em>hàm mất mát</em> dạng MSE như sau:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{eqnarray}\text{MSE}(\mathbf{x}, \mathbf{w}) &amp; = &amp; \frac{1}{2N}\sum_{i=1}^{N} (y_i-\hat{y_i})^2 \\
&amp; = &amp; \frac{1}{2N}\sum_{i=1}^{N} (y_i-\mathbf{w}^{\intercal}\mathbf{x})^2
\end{eqnarray}\end{split}\]</div>
<p>Theo phương pháp điều chuẩn, chúng ta sẽ cùng cộng thêm một phần tử gọi là thành phần điều chuẩn (<em>regularization term</em>) vào hàm MSE.</p>
<div class="math notranslate nohighlight">
\[\begin{eqnarray}\text{MSE}(\mathbf{x}, \mathbf{w}) &amp; = &amp; \frac{1}{2N}\sum_{i=1}^{N} (y_i-\mathbf{w}^{\intercal}\mathbf{x})^2 + \underbrace{\theta ~\text{R}(\mathbf{w})}_{\text{regularization term}}
\end{eqnarray}\]</div>
<p><em>Thành phần điều chuẩn</em> được cộng thêm chủ yếu là một hàm norm chuẩn <span class="math notranslate nohighlight">\(L_2\)</span> hoặc <span class="math notranslate nohighlight">\(L_1\)</span> của véc tơ trọng số <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> của mô hình và <span class="math notranslate nohighlight">\(\theta &gt; 0\)</span> là hệ số điều chuẩn.</p>
<p>Ngoài ra chúng ta có thể áp dụng giá trị trọng số khác nhau cho từng trọng số <span class="math notranslate nohighlight">\(w_i\)</span> thay vì toàn bộ đều là <span class="math notranslate nohighlight">\(\theta\)</span>. Theo phương pháp này, những biến đầu vào mà khiến cho mô hình trở nên phức tạp hơn thì sẽ được gán cho một hệ số <span class="math notranslate nohighlight">\(\theta\)</span> lớn hơn. Ví dụ như hệ số đối với bậc cao như <span class="math notranslate nohighlight">\(x^{15}\)</span> sẽ cao hơn so với bậc thấp như <span class="math notranslate nohighlight">\(x^2, x\)</span> chẳng hạn.</p>
<p><strong>Bài tập:</strong> Hãy lý giải vì sao <em>thành phần điều chuẩn</em> lại có tác dụng làm mô hình ít phức tạp hơn và qua đó giúp giảm thiểu hiện tượng quá khớp.</p>
</div>
</div>
<div class="section" id="phong-tranh-qua-khop-trong-mang-no-ron">
<h2>4.3.2 Phòng tránh quá khớp trong mạng nơ ron<a class="headerlink" href="#phong-tranh-qua-khop-trong-mang-no-ron" title="Permalink to this headline">¶</a></h2>
<p>Quá khớp là hiện tượng thường gặp khi huấn luyện các mạng thần kinh nơ ron. Một phần nguyên nhân là bởi số lượng tham số của một mạng nơ-ron có thể lớn tuỳ ý nên hàm biểu diễn của mạng nơ-ron có khả năng biểu diễn lớn và có độ phức tạp cao. Thậm chí người ta còn chứng minh được rằng mạng nơ-ron có khả năng <strong>xấp xỉ mọi hàm số</strong>. Điều đó cho thấy khả năng biểu diễn của mạng nơ-ron tốt như thế nào và đây là ưu thế giúp cho hiệu suất của mô hình deep learning vượt trội hơn so với các mô hình machine learning trên những bộ dữ liệu có kích thước lớn.</p>
<!-- ![](https://www.seekpng.com/png/full/314-3143166_deep-learning-performance-deep-learning-vs-machine-learning.png) -->
<p><img alt="" src="../_images/dl_vs_ml.png" /></p>
<p><strong>Hình 3:</strong> Hiệu suất của mô hình deep learning so với mô hình machine learning theo sự thay đổi của kích thước bộ dữ liệu. Source: <a class="reference external" href="https://www.analyticsvidhya.com/blog/2017/04/comparison-between-deep-learning-machine-learning/">Comparision between DL vs ML</a></p>
<p>Đối với những bộ dữ liệu có kích thước nhỏ thì hiệu suất giữa mạng nơ ron và các mô hình machine learning truyền thống như Logistic, kNN, SVM, Decision Tree,… không mấy khác biệt nhưng khi các bộ dữ liệu có kích thước lớn thì mạng nơ ron sẽ có hiệu suất vượt trội. Trong nhiều trường hợp bộ dữ liệu có kích thước quá nhỏ, sử dụng mạng nơ ron sẽ dẫn tới hiện tượng quá khớp. Những phương pháp bên dưới sẽ rất hữu ích để phòng tránh hiện tượng này.</p>
<div class="section" id="phuong-phap-dung-som-early-stopping">
<h3>4.3.2.1. Phương pháp dừng sớm (<em>Early stopping</em>)<a class="headerlink" href="#phuong-phap-dung-som-early-stopping" title="Permalink to this headline">¶</a></h3>
<p>Khi huấn luyện càng lâu thì giá trị <em>hàm mất mát</em> của mô hình trên tập huấn luyện càng nhỏ và mô hình có xu hướng khớp tốt dữ liệu trên tập huấn luyện hơn. Mặc dù sai số trên tập huấn luyện có xu hướng giảm dần theo thời gian nhưng trên <em>tập kiểm định</em> (<em>validation data</em>) điều này sẽ chưa chắc là đúng. Lý do là vì thời điểm mô hình đạt tới một độ phức tạp nhất định nó sẽ không còn khái quát hoá tốt dữ liệu kiểm định (hãy nhớ đến phương trình hồi qui bậc 15 ở ví dụ về quá khớp và vị khớp). Như vậy trên <em>tập kiểm định</em> tới một giai đoạn epoch nào đó sai số sẽ tăng lên.</p>
<p>Phương pháp dừng sớm sẽ xác định đâu là epoch được lựa chọn làm điểm dừng phù hợp căn cứ vào sai số trên <em>tập kiểm định</em>. Đó là thời điểm mà sai số trên <em>tập kiểm định</em> bắt đầu có xu hướng tăng lên. Khi đó một quyết định dừng sớm quá trình huấn luyện sẽ giúp tránh hiện tượng <em>quá khớp</em>.</p>
<!-- ![](https://www.researchgate.net/profile/Tuan-Ho-Le-2/publication/283697186/figure/fig3/AS:348490979921923@1460098132631/Early-stopping-method.png) -->
<p><img alt="" src="../_images/Early-stopping-method.png" /></p>
<p><strong>Hình 4:</strong> Phương pháp dừng sớm (<em>early stopping</em>) được thể hiện qua sai số trên tập kiểm định bắt đầu tăng lên tại một epoch nào đó, chẳng hạn như điểm dừng (<em>stop training</em>) trong hình. Theo qui luật tổng quát, trong dài hạn sai số trên tập huấn luyện có xu hướng giảm dần theo epoch bởi quá trình huấn luyện chúng ta luôn tìm cách tối thiểu <em>hàm mất mát</em> trên <strong>tập huấn luyện</strong>.</p>
<p>Phương pháp dừng sớm thường được áp dụng trong quá trình huấn luyện các mô hình deep learning không chỉ giảm thiểu <em>quá khớp</em> mà còn tiết kiệm chi phí huấn luyện. Để tìm được vị trí dừng phù hợp chúng ta sẽ kiểm tra mức độ gia tăng của sai số trên <em>tập kiểm tra</em>. Điều kiện dừng được thiết lập là ngưỡng gia tăng <em>hàm mất mát</em> của epoch sau so với epoch trước lớn hơn <span class="math notranslate nohighlight">\(\Delta_{error}\)</span>. Trong quá trình huấn luyện chúng ta cũng cần liên tục lưu lại các <em>checkpoint</em> cho mô hình sau mỗi epoch cho tới khi đạt được điểm dừng.</p>
</div>
<div class="section" id="phuong-phap-dropout-dropout">
<h3>4.3.2.2. Phương pháp dropout (<em>Dropout</em>)<a class="headerlink" href="#phuong-phap-dropout-dropout" title="Permalink to this headline">¶</a></h3>
<p>Phương pháp dropout sẽ tìm cách làm đơn giản hoá mô hình dự báo thông qua việc loại bỏ một số trọng số của mô hình thông qua mẹo thiết lập giá trị của chúng về 0 trong một vài lượt huấn luyện. Các tham số được lựa chọn để loại bỏ là ngẫu nhiên theo một tỷ lệ được xác định sẵn trên các layers được chọn, thường là những layers tại vị trí đầu tiên hoặc vị trí cuối cùng. Quá trình suy luận (<em>inference</em>) thì chúng ta sẽ lấy toàn bộ trọng số của mô hình mà không loại bỏ.</p>
<!-- ![](https://www.tech-quantum.com/wp-content/uploads/2018/11/1_iWQzxhVlvadk6VAJjsgXgg1.png) -->
<p><img alt="" src="../_images/dropout.png" /></p>
<p><strong>Hình 5:</strong> Phương pháp dropout được áp dụng trên mạng nơ ron. Mạng nơ ron sẽ bao gồm nhiều layers mỗi layer gồm nhiều units. Các trọng số mô hình được thể hiện bởi một mũi tên liên kết units giữa các layers. Hình bên phải là toàn bộ mạng được sử dụng trong quá trình <em>suy luận</em>. Trong khi hình bên trái là áp dụng dropout trên các layers của mạng và chỉ được áp dụng trong quá trình <em>huấn luyện</em>. Trọng số mô hình bị loại bỏ được thể hiện qua những kết nối mũi tên bị xoá bỏ đi. Mô hình mới được tạo thành để huấn luyện sẽ trở nên thưa hơn, đồng thời mức độ phức tạp giảm nhằm giúp giảm <em>quá khớp</em>. Ngoài ra việc lựa chọn tham số để loại bỏ là ngẫu nhiên nên kiến trúc mô hình dropout được biến đổi rất đa dạng. Như vậy mô hình sau cùng thu được về bản chất là một sự kết hợp (<em>ensembling</em>) của các mô hình dự báo.</p>
<p>Mỗĩ một lượt huấn luyện chúng ta loại bỏ một tỷ lệ thiểu số các trọng số mô hình thì sẽ tạo ra một mô hình mới ít phức tạp hơn. Như vậy mô hình được huấn luyện theo kỹ thuật này sẽ là kết hợp của rất nhiều các mô hình biến thể sau dropout và chúng ta có thể xem chúng như là một phương pháp kết hợp mô hình (<em>ensemble model</em>) nhằm giảm <em>quá khớp</em>.</p>
<p>Trong mạng nơ ron thì dropout thường được áp dụng tại các vị trí layers đầu tiên và layers cuối cùng. Khi áp dụng chúng ta cần xác định một tỷ lệ dropout rate qui định phần trăm các trọng số sẽ bị loại bỏ khỏi từng layer. Đối với layer đầu tiên thì các low-level features còn thô (chưa tốt) nên tỷ lệ loại bỏ có thể được thiết lập cao hơn chẳng hạn từ 0.7-0.8, nhưng đối với layer cuối cùng là những high-level features tốt và cần thiết cho quá trình dự báo nên dropout rate được thiết lập thấp hơn (từ 0.1-0.5).</p>
</div>
</div>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="xu-ly-hien-tuong-vi-khop">
<h1>4.4. Xử lý hiện tượng vị khớp<a class="headerlink" href="#xu-ly-hien-tuong-vi-khop" title="Permalink to this headline">¶</a></h1>
<p>Để xử lý hiện tượng vị khớp chúng ta có thể tập trung vào dữ liệu hoặc mô hình. Cùng phân tích những phương pháp này như bên dưới.</p>
<div class="section" id="bo-sung-du-lieu-cho-mo-hinh">
<h2>4.4.1. Bổ sung dữ liệu cho mô hình<a class="headerlink" href="#bo-sung-du-lieu-cho-mo-hinh" title="Permalink to this headline">¶</a></h2>
<p>Bổ sung dữ liệu cho mô hình là một chiến lược lâu dài và tốn kém hơn so với việc thay đổi kiến trúc. Nhưng dường như nó lại là phương pháp mang lại hiệu quả lớn hơn so với thay đổi kiến trúc.</p>
<p>Sở dĩ chúng ta nói bổ sung dữ liệu tốn kém hơn so với thay đổi kiến trúc là bởi nguồn open source của các mô hình deep learning hiện tại rất dồi dào. Chúng ta có thể dễ dàng tham khảo và kế thừa lại trong các tác vụ của mình trong một thời gian ngắn. Quá trình này đơn giản và tốn ít công sức hơn nhiều so với đầu tư công phu vào làm dữ liệu.</p>
<p>Bên cạnh đó các mô hình deep learning nhỏ với kích thước vài triệu tham số cũng có thể đủ để biểu diễn tốt các tập dữ liệu lớn. Khi dữ liệu được cải thiện và bổ sung thì hiệu suất của những mô hình có backbone nhẹ vài triệu tham số có thể vượt xa những backbone nặng vài chục triệu hoặc thậm chí vài trăm triệu tham số nhưng chỉ được huấn luyện trên những bộ dữ liệu nhỏ. Như vậy ở thời điểm dữ liệu đang còn thiếu và ít thì tập trung vào dữ liệu sẽ mang lại hiệu suất lớn hơn so với tập trung vào cải thiện kiến trúc mô hình.</p>
<p>Quá trình bổ sung dữ liệu cho mô hình sẽ bao gồm thu thập và gán nhãn dữ liệu. Những dữ liệu cần thu thập nên bao quát những tình huống hiếm có (<em>edge cases</em>) mà mô hình cần bao phủ được để cải thiện chất lượng của chúng. Trong giai đoạn gán nhãn, để tiết kiệm chi phí gán nhãn thì chúng ta có thể sử dụng các mô hình được huấn luyện trên những backbone mạnh để có pretrained-label tốt hơn. Mô hình được dùng cho pretrained-label có thể rất lớn mà không cần quan tâm tới chi phí tính toán và khả năng triển khai trên thiết bị edge devices vì mục tiêu của chúng không phải là triển khai những mô hình này mà chỉ là tạo ra nhãn gợi ý tốt cho gán nhãn.</p>
</div>
<div class="section" id="tang-cuong-du-lieu-augumentation">
<h2>4.4.2. Tăng cường dữ liệu (Augumentation)<a class="headerlink" href="#tang-cuong-du-lieu-augumentation" title="Permalink to this headline">¶</a></h2>
<p>Tăng cường dữ liệu là một nguyên tắc bổ sung dữ liệu với một chi phí rẻ. Theo phương pháp này, từ một quan sát chúng ta sẽ nhân bản thành nhiều quan sát bằng cách áp dụng các kỹ thuật biến đổi mà giá trị sau biến đổi của chúng có thể mô phỏng lại một cách tương đối chính xác và đa dạng các trường hợp thực tế.</p>
<p><strong>Tăng cường dữ liệu trong Computer Vision và NLP</strong></p>
<p>Trong xử lý ảnh chúng ta có thể áp dụng phương pháp tăng cường dữ liệu như: <em>Random Augumentation, Cutout, CutMix, Mixup, …</em></p>
<p><img alt="" src="https://i.imgur.com/sqj4GxG.png" /></p>
<p><strong>Hình 6</strong>: Các hình ảnh minh hoạ <code class="docutils literal notranslate"><span class="pre">Random</span> <span class="pre">Augumentation</span></code>. Source: <a class="reference external" href="https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-deep-learning-tips-and-tricks">Augumentation Image - Stanford Edu</a></p>
<ul class="simple">
<li><p>Random Augumentation: Là việc áp dụng tập hợp các phương pháp biến đổi hình ảnh như Shift (dịch chuyển ảnh), Rotation (xoay ảnh), Bright Contrast (tạo tương phản màu sắc),… một cách ngẫu nhiên. Phương pháp này sẽ tạo ra thay đổi trên ảnh đầu vào mà không thay đổi nhãn của ảnh.</p></li>
</ul>
<p><img alt="" src="https://i.imgur.com/JgaeJew.png" /></p>
<p><strong>Hình 7</strong>: Các hình ảnh minh hoạ <code class="docutils literal notranslate"><span class="pre">Cutout,</span> <span class="pre">Mixup</span></code> và <code class="docutils literal notranslate"><span class="pre">cutmix</span></code>. Source: <a class="reference external" href="https://arxiv.org/abs/1905.04899v1">Paper: CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features</a></p>
<ul class="simple">
<li><p>Cutout: Phương pháp này sẽ tạo ra ảnh mới, không thay đổi nhãn của ảnh bằng cách loại bỏ những vùng ô vuông trên một ảnh một cách ngẫu nhiên. Tỷ lệ diện tích các vùng ô vuông này chiếm một phần nhỏ diện tích toàn bộ ảnh.</p></li>
<li><p>Mixup: Đây là phương pháp tạo ra những nhãn mềm (<em>soft label</em>) cho ảnh bằng cách hỗn hợp hai bức ảnh thuộc về hai classes khác nhau bằng một kết hợp tuyến tính giữa chúng. Nhãn mới được tạo thành sẽ là một kết hợp tuyến tính giữa chúng, nhãn này có giá trị thể hiện sự lưỡng lự giữa các class khi không nghiêng hẳn về một class nào.</p></li>
</ul>
<p>Ví dụ nếu bạn có <span class="math notranslate nohighlight">\(\mathcal{I}_0\)</span> là ảnh mèo nhãn 0 và <span class="math notranslate nohighlight">\(\mathcal{I}_{1}\)</span> là ảnh chó nhãn 1 thì ảnh mới tạo thành sẽ lấy 90% thông tin từ mèo và 10% thông tin từ ảnh chó theo kết hợp tuyến tính:</p>
<div class="math notranslate nohighlight">
\[\mathcal{I}_{aug} = 0.9 \times \mathcal{I}_0 + 0.1 \times \mathcal{I}_{1}\]</div>
<p>Ảnh mới <span class="math notranslate nohighlight">\(\mathcal{I}_{aug}\)</span> có nhãn là:</p>
<div class="math notranslate nohighlight">
\[y_{aug} = 0.9 \times y_0 + 0.1 \times y_1 = 0.9\]</div>
<p>Nhãn này không bị cứng nhắc, tức là chỉ thuộc về một trong hai giá trị <span class="math notranslate nohighlight">\(\{0, 1\}\)</span> mà có thể thay đổi đa dạng trong khoảng <span class="math notranslate nohighlight">\([0, 1]\)</span> nên tạo ra một sự linh hoạt nhất định về nhãn. Do đó ta gọi chúng là nhãn mềm.</p>
<ul class="simple">
<li><p>CutMix: Phương pháp này vừa là kết hợp giữa Cutout và Mixup. Theo đó chúng ta thay thế những ô vuông trên một ảnh bằng những ô vuông có cùng diện tích của một ảnh khác thuộc các nhãn còn lại. Nhãn mới được tạo thành cũng là nhãn mềm được kết hợp tuyến tính từ hai nhãn.</p></li>
</ul>
<p>Trong NLP chúng ta có thể tăng cường dữ liệu bằng cách thay thế một số từ trong câu tại các vị trí ngẫu nhiên bằng những từ đồng nghĩa. Đối với bài toán phân loại văn bản thì đảo lộn vị trí các câu trong đoạn văn để tạo thành đoạn văn mới có cùng nhãn. Việc tận dụng các mô hình dịch máy cũng có thể giúp tạo ra một phương pháp augumentation hiệu quả. Theo phương pháp này, từ một câu gốc A Tiếng Việt chúng ta có thể dịch sang câu B Tiếng Anh và sau đó dịch ngược trở lại từ câu B Tiếng Anh sang câu A’ Tiếng Việt là một biến thể có nội dung tương tự như câu gốc A.</p>
<p><strong>Tăng cường dữ liệu đối với tabular data</strong></p>
<p>Các bài toán phân loại đối với dữ liệu dạng bảng (<em>tabular dataset</em>) thường sử dụng những phương pháp sinh mẫu như SMOTE (<em>Synthetic Minority Oversampling Technique</em>), Random Sampling để tăng cường dữ liệu. Sinh mẫu thường được áp dụng và tỏ ra hiệu quả trong các trường hợp xảy ra hiện tượng mất cân bằng dữ liệu trầm trọng, khi đó chúng ta thường áp dụng sinh mẫu trên những nhóm thiểu số.</p>
<ul class="simple">
<li><p>Random Sampling: Chúng ta sẽ lấy mẫu lặp lại một cách ngẫu nhiên. Các mẫu được sinh ra là các sao chép ngẫu nhiên từ những mẫu cũ.</p></li>
<li><p>SMOTE: Phương pháp này sẽ tạo ra những mẫu mới dựa trên những phân phối của những mẫu gần nó nhất. Mẫu mới được tạo thành có thể được lấy bằng trung bình có trọng số hoặc không có trọng số trên <span class="math notranslate nohighlight">\(k\)</span> mẫu cùng nhãn gẫn nhất.</p></li>
</ul>
</div>
<div class="section" id="su-dung-thuat-toan-phuc-tap-hon">
<h2>4.4.3. Sử dụng thuật toán phức tạp hơn<a class="headerlink" href="#su-dung-thuat-toan-phuc-tap-hon" title="Permalink to this headline">¶</a></h2>
<p>Phương pháp này là một hướng cải thiện dựa trên mô hình. Đối với những bộ dữ liệu kích thước lớn mà mô hình có hiệu suất thấp thì chúng ta có thể chuyển sang những thuật toán phức tạp hơn.</p>
<p>Trong các mô hình phân loại thuộc lớp mô hình học có giám sát của machine learning thì những mô hình được coi là phức tạp hơn thường là <code class="docutils literal notranslate"><span class="pre">Random</span> <span class="pre">Forest,</span> <span class="pre">Decision</span> <span class="pre">Tree,</span> <span class="pre">MLP,</span> <span class="pre">SVM</span></code> và ít phức tạp là <code class="docutils literal notranslate"><span class="pre">Logistic,</span> <span class="pre">Naive</span> <span class="pre">Bayes,</span> <span class="pre">k-NN</span></code>.</p>
<p>Đối với Deep Learning thì các kiến trúc phức tạp hơn được thể hiện qua độ sâu lớn hơn, số lượng tham số lớn hơn. Ngày nay cùng với sự phát triển mạnh mẽ của nghiên cứu, thực nghiệm và dữ liệu lớn khiến cho các kiến trúc của Deep Learning trở nên vô cùng dồi dào và đa dạng. Do đó thật khó để chúng ta nói đâu là một backbone hiệu quả nhất bởi thứ hạng chúng thường thay đổi theo thời gian. Thứ hạng chính của những backbones có thể được tìm kiếm tại leaderboard trên các tập dataset chuẩn như <a class="reference external" href="https://paperswithcode.com/sota/image-classification-on-imagenet">ImageNet Leader Board</a>.</p>
<p>Trong NLP thì các lớp mô hình pretrain chủ yếu là BERT và các biến thể của BERT được công khai trên <a class="reference external" href="https://huggingface.co/models">huggingface hub</a>. Tại đây bạn có thể tìm được các mô hình biểu diễn ngôn ngữ tốt cho cả mono-language và multi-language. Mô hình pretrain cho Tiếng Việt nổi tiếng là <a class="reference external" href="https://github.com/VinAIResearch/PhoBERT">PhoBERT</a>.</p>
</div>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="tong-ket">
<h1>4.5. Tổng kết<a class="headerlink" href="#tong-ket" title="Permalink to this headline">¶</a></h1>
<p><em>quá khớp</em> và <em>vị khớp</em> là những nguyên nhân chính khiến mô hình không còn đúng khi áp dụng vào bài toán dự báo thực tế. Hiểu và nắm vững những đặc điểm về chúng và cách thức khắc phục sẽ giúp tạo ra những mô hình tốt hơn. Như vậy chương này chúng ta đã thu thập thêm được những kiến thức mới:</p>
<ol class="simple">
<li><p>Độ chệch và độ biến động là gì ? Đánh đổi giữa độ chệch và độ biến động trong quá trình xây dựng mô hình.</p></li>
<li><p>Các hiện tượng <em>quá khớp</em> và <em>vị khớp</em> cùng hậu quả của chúng.</p></li>
<li><p>Các phương pháp giảm thiểu <em>quá khớp</em> đối với mô hình machine learning và mạng nơ ron.</p></li>
<li><p>Các phương pháp giảm thiểu <em>vị khớp</em>.</p></li>
</ol>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="bai-tap-tham-khao">
<h1>4.6. Bài tập tham khảo<a class="headerlink" href="#bai-tap-tham-khao" title="Permalink to this headline">¶</a></h1>
<ol class="simple">
<li><p><em>quá khớp</em> là gì ? <em>vị khớp</em> là gì ? Khác biệt chính giữa chúng là gì ?</p></li>
<li><p>Ảnh hưởng của hiện tượng <em>quá khớp</em> và <em>vị khớp</em> là gì ?</p></li>
<li><p>Khi mô hình xảy ra hiện tượng <em>quá khớp</em>, có những chiến lược nào để khắc phục?</p></li>
<li><p>Khi mô hình xảy ra hiện tượng <em>vị khớp</em>, chúng ta cần sử dụng chiến lược gì?</p></li>
<li><p>Làm sao để kiểm tra một mô hình có gặp hiện tượng <em>quá khớp</em> hay không nếu bạn chỉ có một tập huấn luyện.</p></li>
<li><p>Trong phương pháp dừng sớm (<em>early stopping</em>) thì điều kiện dừng là gì ?</p></li>
<li><p>Phương pháp điều chuẩn (<em>regularization</em>) sẽ làm gì để giảm bớt <em>quá khớp</em>?</p></li>
<li><p>Những phương pháp học tăng cường dữ liệu chính đối với các mô hình phân loại ảnh là gì ?</p></li>
<li><p>Hãy xây dựng và huấn luyện một mô hình phân loại trên bộ dữ liệu <a class="reference external" href="https://github.com/phamdinhkhanh/datasets/blob/master/breastcancer_training.csv">breast cancer</a>. Nhận xét xem mô hình đang gặp hiện tượng <em>quá khớp</em> hay <em>vị khớp</em>?</p></li>
<li><p>Thực hành khắc phục hiện tượng xảy ra của mô hình ở câu 9.</p></li>
</ol>
</div>
<div class="tex2jax_ignore mathjax_ignore section" id="tai-lieu-tham-khao">
<h1>4.7. Tài liệu tham khảo<a class="headerlink" href="#tai-lieu-tham-khao" title="Permalink to this headline">¶</a></h1>
<ol class="simple">
<li><p><a class="reference external" href="https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229">Understand the bias and variance trade-off</a></p></li>
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff">Bias and Variance trade off</a></p></li>
<li><p><a class="reference external" href="https://www.youtube.com/watch?v=u73PU6Qwl1I">Andrew Ng - Problem of Overfitting</a></p></li>
<li><p><a class="reference external" href="https://machinelearningmastery.com/Overfitting-and-underfitting-with-machine-learning-algorithms/">Overfitting and Underfitting with machine learning algorithms</a></p></li>
</ol>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./ch_ml"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="index_OvfAndUdf.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">4. Độ chệch (<em>bias</em>) và phương sai (<em>variance</em>)</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="index_ModelMetric.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">5. Thước đo mô hình phân loại</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Pham Dinh Khanh<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>